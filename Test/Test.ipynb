{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eacb5fc8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e983ba8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6ea6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tejomaya_ETL_pipeline/\n",
    "# ‚îÇ\n",
    "# ‚îú‚îÄ‚îÄ agents/\n",
    "# ‚îÇ   ‚îú‚îÄ‚îÄ searching_agent.py\n",
    "# ‚îÇ   ‚îú‚îÄ‚îÄ parsing_agent.py\n",
    "# ‚îÇ   ‚îú‚îÄ‚îÄ run_agents.py\n",
    "# ‚îÇ\n",
    "# ‚îú‚îÄ‚îÄ scheduler/\n",
    "# ‚îÇ   ‚îî‚îÄ‚îÄ scheduler.py\n",
    "# ‚îÇ\n",
    "# ‚îú‚îÄ‚îÄ api/\n",
    "# ‚îÇ   ‚îî‚îÄ‚îÄ main.py\n",
    "# ‚îÇ\n",
    "# ‚îú‚îÄ‚îÄ data/\n",
    "# ‚îÇ   ‚îú‚îÄ‚îÄ output_excels/\n",
    "# ‚îÇ   ‚îî‚îÄ‚îÄ weekly_sebi_downloads.xlsx\n",
    "# ‚îÇ\n",
    "# ‚îú‚îÄ‚îÄ logs/\n",
    "# ‚îÇ   ‚îú‚îÄ‚îÄ searching_agent.log\n",
    "# ‚îÇ   ‚îú‚îÄ‚îÄ parsing_agent.log\n",
    "# ‚îÇ   ‚îú‚îÄ‚îÄ scheduler.log\n",
    "# ‚îÇ\n",
    "# ‚îî‚îÄ‚îÄ requirements.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e44c4d",
   "metadata": {},
   "source": [
    "# date - Monday to Sunday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ddca6260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Range for 0 week(s) back: 2025-11-17 to 2025-11-18\n",
      "        Date  Yes/No\n",
      "6 2025-11-17     NaN\n",
      "7 2025-11-18     NaN\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "excel_path = \"Test_date.xlsx\"\n",
    "date_column = \"Date\"\n",
    "\n",
    "today = datetime.today()\n",
    "\n",
    "# Get THIS week's Monday (00:00)\n",
    "this_monday = (today - timedelta(days=today.weekday())).replace(\n",
    "    hour=0, minute=0, second=0, microsecond=0\n",
    ")\n",
    "\n",
    "weeks_back = 0   # <--- change this to check previous weeks\n",
    "\n",
    "# Target week's Monday\n",
    "target_monday = this_monday - timedelta(weeks=weeks_back)\n",
    "\n",
    "# Target week's Sunday (23:59:59)\n",
    "target_sunday = (target_monday + timedelta(days=6)).replace(\n",
    "    hour=23, minute=59, second=59, microsecond=999999\n",
    ")\n",
    "\n",
    "# ‚ùó FIX: If this week ‚Üí End at TODAY, not Sunday\n",
    "if weeks_back == 0:\n",
    "    target_sunday = today.replace(\n",
    "        hour=23, minute=59, second=59, microsecond=999999\n",
    "    )\n",
    "\n",
    "print(f\"Range for {weeks_back} week(s) back:\", target_monday.date(), \"to\", target_sunday.date())\n",
    "\n",
    "# Load data\n",
    "df = pd.read_excel(excel_path)\n",
    "\n",
    "# Filter\n",
    "mask = (df[date_column] >= target_monday) & (df[date_column] <= target_sunday)\n",
    "df_week = df[mask]\n",
    "\n",
    "print(df_week)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19bed96e",
   "metadata": {},
   "source": [
    "# scraping script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127bf311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================================\n",
    "# CRITICAL FIX FOR WINDOWS - MUST BE AT THE VERY TOP OF THE SCRIPT\n",
    "# =========================================================================\n",
    "import sys\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "import platform\n",
    "import os\n",
    "\n",
    "if sys.platform.startswith(\"win\"):\n",
    "    asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())\n",
    "    nest_asyncio.apply()\n",
    "# =========================================================================\n",
    "\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from crawl4ai import AsyncWebCrawler\n",
    "from bs4 import BeautifulSoup\n",
    "import aiohttp\n",
    "import base64\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# ---------------- CONFIG -----------------\n",
    "BASE_URL = \"https://www.sebi.gov.in\"\n",
    "LISTING_URL = \"https://www.sebi.gov.in/sebiweb/home/HomeAction.do?doListing=yes&sid=6&ssid=23&smid=0\"\n",
    "CATEGORY = \"SEBI\"\n",
    "SUBFOLDER = \"Press Release\"\n",
    "\n",
    "if platform.system() == \"Windows\":\n",
    "    BASE_PATH = r\"C:\\Users\\Admin\\Downloads\\Tejomaya_pdfs\\Akshayam Data\"\n",
    "else:\n",
    "    BASE_PATH =   \"/Users/admin/Downloads/Tejomaya_pdfs/Akshayam Data\"\n",
    "\n",
    "# ----------------------------------------\n",
    "\n",
    "def ensure_year_month_structure(base_folder, category, subfolder, year, month_full):\n",
    "    subfolder_path = os.path.join(base_folder, category, subfolder)\n",
    "    year_path = os.path.join(subfolder_path, year)\n",
    "    os.makedirs(year_path, exist_ok=True)\n",
    "    month_path = os.path.join(year_path, month_full)\n",
    "    os.makedirs(month_path, exist_ok=True)\n",
    "    return month_path\n",
    "\n",
    "def sanitize_filename(title):\n",
    "    filename = re.sub(r'[\\/\\\\\\:\\*\\?\"<>\\|]', '_', title)\n",
    "    filename = filename.replace(\" \", \"_\")\n",
    "    return filename + \".pdf\"\n",
    "\n",
    "async def download_pdf(session, pdf_url, save_path):\n",
    "    filename = os.path.basename(urlparse(pdf_url).path)\n",
    "    file_path = os.path.join(save_path, filename)\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"‚è≠Ô∏è Skipping (already exists): {file_path}\")\n",
    "        return file_path\n",
    "    \n",
    "    try:\n",
    "        async with session.get(pdf_url) as resp:\n",
    "            if resp.status == 200:\n",
    "                with open(file_path, \"wb\") as f:\n",
    "                    f.write(await resp.read())\n",
    "                print(f\"üì• PDF downloaded: {file_path}\")\n",
    "                return file_path\n",
    "            else:\n",
    "                print(f\"‚ùå Failed to download PDF: {pdf_url}, status: {resp.status}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error downloading PDF: {e}\")\n",
    "    return None\n",
    "\n",
    "def sync_fetch_links(page_number):\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--headless')\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "    titles_and_urls = []\n",
    "    try:\n",
    "        driver.get(LISTING_URL)\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"a.points\"))\n",
    "        )\n",
    "\n",
    "        if page_number > 1:\n",
    "            page_index = page_number - 1\n",
    "            driver.execute_script(f\"searchFormNewsList('n', '{page_index}');\")\n",
    "            WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"a.points\"))\n",
    "            )\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        for link in soup.select(\"a.points[href]\"):\n",
    "            detail_url = link.get(\"href\")\n",
    "            title = link.get_text(strip=True)\n",
    "            if detail_url:\n",
    "                titles_and_urls.append({\n",
    "                    \"title\": title,\n",
    "                    \"detail_url\": urljoin(BASE_URL, detail_url)\n",
    "                })\n",
    "    finally:\n",
    "        driver.quit()\n",
    "    return titles_and_urls\n",
    "\n",
    "\n",
    "async def scrape_sebi_page(page_number: int):\n",
    "    downloaded_files = []\n",
    "    print(f\"\\n--- Processing SEBI Page {page_number} ---\")\n",
    "\n",
    "    links_data = await asyncio.to_thread(sync_fetch_links, page_number)\n",
    "    if not links_data:\n",
    "        print(f\"‚ùå No titles found on page {page_number}\")\n",
    "        return\n",
    "\n",
    "    for link_data in links_data:\n",
    "        title = link_data['title']\n",
    "        detail_url = link_data['detail_url']\n",
    "        print(f\"\\nüîó Opening detail page: {detail_url}\")\n",
    "\n",
    "        async with AsyncWebCrawler() as crawler:\n",
    "            detail_result = await crawler.arun(url=detail_url)\n",
    "        soup_detail = BeautifulSoup(detail_result.html, \"html.parser\")\n",
    "\n",
    "        # Extract date\n",
    "        date_elem = soup_detail.select_one(\"h5\")\n",
    "        if date_elem:\n",
    "            date_str = date_elem.get_text(strip=True)\n",
    "            try:\n",
    "                dt = datetime.strptime(date_str, \"%b %d, %Y\")\n",
    "                year, month_full = str(dt.year), dt.strftime(\"%B\")\n",
    "            except ValueError:\n",
    "                year, month_full = \"UnknownYear\", \"UnknownMonth\"\n",
    "        else:\n",
    "            date_str, year, month_full = \"\", \"UnknownYear\", \"UnknownMonth\"\n",
    "\n",
    "        save_path = ensure_year_month_structure(BASE_PATH, CATEGORY, SUBFOLDER, year, month_full)\n",
    "\n",
    "        # Try finding PDF (iframe or download button)\n",
    "        pdf_url = None\n",
    "        iframe = soup_detail.select_one(\"iframe\")\n",
    "        pdf_button = soup_detail.select_one(\"button#download\")\n",
    "\n",
    "        if iframe and \"file=\" in iframe.get(\"src\", \"\"):\n",
    "            pdf_url = iframe[\"src\"].split(\"file=\")[-1]\n",
    "            if not pdf_url.startswith(\"http\"):\n",
    "                pdf_url = urljoin(BASE_URL, pdf_url)\n",
    "            print(\"‚úÖ PDF found in iframe:\", pdf_url)\n",
    "        elif pdf_button:\n",
    "            pdf_url = detail_url.replace(\".html\", \".pdf\")\n",
    "            print(\"‚úÖ PDF button found, trying direct PDF link:\", pdf_url)\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è No direct PDF found, switching to print-to-PDF mode\")\n",
    "\n",
    "        # ---------------- Handle Download or Print ----------------\n",
    "        file_path = None\n",
    "        if pdf_url:\n",
    "            async with aiohttp.ClientSession() as session:\n",
    "                file_path = await download_pdf(session, pdf_url, save_path)\n",
    "\n",
    "        if not file_path:  # fallback to print-to-PDF\n",
    "            # Keep original page title or generate from URL for natural naming\n",
    "            page_name = os.path.basename(urlparse(detail_url).path).replace(\".html\", \".pdf\")\n",
    "            file_path = os.path.join(save_path, page_name)\n",
    "\n",
    "            options = webdriver.ChromeOptions()\n",
    "            options.add_argument('--headless')\n",
    "            driver = webdriver.Chrome(options=options)\n",
    "            driver.get(detail_url)\n",
    "\n",
    "            # Hide headers/sidebars and keep only main printable content\n",
    "            driver.execute_script(\"\"\"\n",
    "                document.querySelectorAll('header, nav, aside, footer, .sidebar, .topnav').forEach(e => e.style.display='none');\n",
    "                let main = document.querySelector('#printableArea');\n",
    "                if(main) main.style.width = '100%';\n",
    "            \"\"\")\n",
    "            \n",
    "            pdf = driver.execute_cdp_cmd(\"Page.printToPDF\", {\"printBackground\": True})\n",
    "            pdf_data = base64.b64decode(pdf['data'])\n",
    "            with open(file_path, \"wb\") as f:\n",
    "                f.write(pdf_data)\n",
    "            driver.quit()\n",
    "            print(f\"üñ®Ô∏è Printed to PDF (natural name): {file_path}\")\n",
    "\n",
    "\n",
    "        downloaded_files.append({\n",
    "            \"Title\": title,\n",
    "            \"Date\": date_str,\n",
    "            \"PDF_Path\": os.path.abspath(file_path)\n",
    "        })\n",
    "\n",
    "    # Save Excel for the page\n",
    "    if downloaded_files:\n",
    "        excel_file = os.path.join(BASE_PATH, f\"downloaded_pdfs_page{page_number}.xlsx\")\n",
    "        df = pd.DataFrame(downloaded_files)\n",
    "        df.to_excel(excel_file, index=False)\n",
    "        print(f\"\\n‚úÖ Excel saved: {os.path.abspath(excel_file)}\")\n",
    "    else:\n",
    "        print(f\"No PDFs downloaded for page {page_number}, Excel not created.\")\n",
    "\n",
    "\n",
    "async def main():\n",
    "    for page in range(1, 2):  # Change range as needed\n",
    "        await scrape_sebi_page(page)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c946619c",
   "metadata": {},
   "source": [
    "# client summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e26074b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from dotenv import load_dotenv\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain.prompts import PromptTemplate\n",
    "from openpyxl import load_workbook, Workbook\n",
    "import torch\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "# ---------------------- Setup ----------------------\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    handlers=[logging.StreamHandler()]\n",
    ")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\"No languages specified, defaulting to English.\")\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "load_dotenv()\n",
    "\n",
    "# ---------------------- GPU Detection ----------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "os.environ[\"OLLAMA_USE_GPU\"] = \"1\" if device.type == \"cuda\" else \"0\"\n",
    "logging.info(f\"üí™ Using device: {device}\")\n",
    "logging.info(\"Ollama GPU status: \" + (\"Enabled\" if os.environ.get(\"OLLAMA_USE_GPU\") == \"1\" else \"Disabled\"))\n",
    "\n",
    "# ---------------------- Initialize LLM ----------------------\n",
    "llm = OllamaLLM(\n",
    "    model=\"mistral:latest\",\n",
    "    device=device.type\n",
    ")\n",
    "\n",
    "chunk_prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    You are analyzing a regulatory or financial document.\n",
    "    Summarize the following section in a **concise and factual** manner (2 sentences max).\n",
    "\n",
    "    Focus only on:\n",
    "    - The key regulatory or policy action (amendment, relaxation, proposal, or update)\n",
    "    - Who or what it affects\n",
    "    - Any critical dates or deadlines mentioned\n",
    "    \n",
    "    Avoid:\n",
    "    - Legal citations, section numbers, or procedural details\n",
    "    - Repetition of headers, page numbers, or contact information\n",
    "    - Explanations, reasoning, or generic filler text\n",
    "\n",
    "    Chunk Text:\n",
    "    {text}\n",
    "\n",
    "    Short factual summary:\n",
    "    \"\"\",\n",
    "    input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "final_prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    You are a senior communications analyst summarizing official regulatory or legal documents\n",
    "    for a client update. Create a concise, plain-language summary.\n",
    "\n",
    "    Guidelines:\n",
    "    - Limit strictly to 5‚Äì6 sentences.\n",
    "    - Focus only on the core purpose, key changes, and who it applies to.\n",
    "    - Avoid mentioning:\n",
    "        ‚Ä¢ Emails, phone numbers, or submission instructions\n",
    "        ‚Ä¢ Application processes, forms, or deadlines\n",
    "        ‚Ä¢ Section or clause numbers unless critical for understanding\n",
    "        ‚Ä¢ Legal or technical jargon (use simple terms instead)\n",
    "    - Keep the tone professional and factual, not legalistic.\n",
    "    - Do NOT repeat phrases or include disclaimers.\n",
    "\n",
    "    Example Output:\n",
    "    \"SEBI has proposed amendments to the Listing Regulations to simplify compliance and ensure\n",
    "    new securities issued through splits or schemes are in dematerialized form. The consultation\n",
    "    seeks stakeholder feedback to improve market transparency and efficiency.\"\n",
    "\n",
    "    Text:\n",
    "    {text}\n",
    "\n",
    "    Short Client Summary:\n",
    "    \"\"\",\n",
    "    input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "\n",
    "# ---------------------- Folders ----------------------\n",
    "OUTPUT_EXCEL_DIR = Path.cwd() / \"output_excels\"\n",
    "OUTPUT_EXCEL_DIR.mkdir(exist_ok=True)\n",
    "BASE_PDF_FOLDER_NAME = \"All Data\"\n",
    "\n",
    "# ---------------------- PDF Cleaning ----------------------\n",
    "def clean_pdf_text(text: str) -> str:\n",
    "    \"\"\"Remove headers, footers, page numbers, and repeated SEBI-like content.\"\"\"\n",
    "    # Split by page (form feed or manual detection)\n",
    "    pages = re.split(r'\\f+', text)\n",
    "    cleaned_pages = []\n",
    "\n",
    "    for page in pages:\n",
    "        lines = page.splitlines()\n",
    "        # Remove first 1‚Äì2 and last 1‚Äì2 lines (common headers/footers)\n",
    "        if len(lines) > 4:\n",
    "            lines = lines[2:-2]\n",
    "        page_text = \"\\n\".join(lines)\n",
    "\n",
    "        # Remove page numbers like 'Page 3 of 10'\n",
    "        page_text = re.sub(r'Page\\s*\\d+\\s*(of\\s*\\d+)?', '', page_text, flags=re.IGNORECASE)\n",
    "\n",
    "        # Remove repeated SEBI headers/footers\n",
    "        page_text = re.sub(r'(Securities and Exchange Board of India|Consultation Paper|Master Circular)', '', page_text, flags=re.IGNORECASE)\n",
    "\n",
    "        cleaned_pages.append(page_text.strip())\n",
    "\n",
    "    # Merge pages\n",
    "    cleaned_text = \"\\n\".join(cleaned_pages)\n",
    "\n",
    "    # Remove extra spaces/newlines\n",
    "    cleaned_text = re.sub(r'\\n{2,}', '\\n', cleaned_text)\n",
    "    cleaned_text = re.sub(r'\\s{2,}', ' ', cleaned_text)\n",
    "\n",
    "    return cleaned_text.strip()\n",
    "\n",
    "# ---------------------- Language Detection ----------------------\n",
    "def detect_language(text: str) -> str:\n",
    "    devanagari_chars = re.findall(r'[\\u0900-\\u097F]', text)\n",
    "    latin_chars = re.findall(r'[A-Za-z]', text)\n",
    "    d_len, l_len = len(devanagari_chars), len(latin_chars)\n",
    "    total = d_len + l_len\n",
    "    if total == 0:\n",
    "        return \"unknown\"\n",
    "    hindi_ratio = d_len / total\n",
    "    english_ratio = l_len / total\n",
    "    if hindi_ratio > 0.3 and english_ratio > 0.3:\n",
    "        return \"mixed\"\n",
    "    elif hindi_ratio > 0.3:\n",
    "        return \"hindi\"\n",
    "    elif english_ratio > 0.3:\n",
    "        return \"english\"\n",
    "    else:\n",
    "        return \"unknown\"\n",
    "\n",
    "# ---------------------- English Filtering ----------------------\n",
    "def filter_english_text(text: str) -> str:\n",
    "    lines = text.split('\\n')\n",
    "    english_lines = [line for line in lines if re.search(r'[A-Za-z]', line) and not re.search(r'[\\u0900-\\u097F]', line)]\n",
    "    return \"\\n\".join(english_lines)\n",
    "\n",
    "# ---------------------- Index Extraction ----------------------\n",
    "def extract_indexing_from_first_page(pdf_path: str) -> str:\n",
    "    try:\n",
    "        first_page_data = partition_pdf(\n",
    "            filename=str(pdf_path),\n",
    "            strategy=\"fast\",\n",
    "            include_page_breaks=False,\n",
    "            starting_page_number=1,\n",
    "            max_pages=1\n",
    "        )\n",
    "        first_page_text = \"\\n\".join(str(el) for el in first_page_data if el)\n",
    "        #pattern = r'\\b(?:REGD\\.?\\s*NO\\.?\\s*[A-Z.\\s\\-]*\\d+(?:/\\d+)*|IBBI/\\d{4}-\\d{2}/GN/REG\\d{3})\\b'\n",
    "        pattern = r\"\"\"\n",
    "        \\b(\n",
    "            REGD\\.?\\s*No\\.?\\s*[A-Z.\\-\\s]*\\d+(?:/\\d+)* |\n",
    "            (?:CG|DL|MH|HR|UP|GJ|TN|RJ|KL|KA|WB|PB|CH|UK|AS|OR|BR|AP|TS|HP|GA|JK|NL|MN|TR|SK|AR)-[A-Z]{2}-E-\\d{8}-\\d+ |\n",
    "            No\\.?\\s*[A-Z/.\\-]*\\d+(?:/\\d+)* |\n",
    "            SEBI/[A-Z]{2,}/\\d{2}/\\d{2} |\n",
    "            S\\.O\\.\\s*\\d+\\(E\\) |\n",
    "            IBBI/\\d{4}-\\d{2}/GN/REG\\d+\n",
    "        )\\b\n",
    "        \"\"\"\n",
    "        matches = re.findall(pattern, first_page_text, flags=re.VERBOSE)\n",
    "\n",
    "        #matches = re.findall(pattern, first_page_text)\n",
    "        return \", \".join(matches) if matches else \"NA\"\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"‚ö†Ô∏è Could not extract indexing info from {pdf_path}: {e}\")\n",
    "        return \"NA\"\n",
    "\n",
    "# ---------------------- PDF Extraction ----------------------\n",
    "def extract_pdf_text(pdf_path: str) -> (str, str, str):\n",
    "    pdf_path = Path(pdf_path).resolve()\n",
    "    indexing = extract_indexing_from_first_page(pdf_path)\n",
    "\n",
    "    raw_data = partition_pdf(filename=str(pdf_path), strategy=\"fast\", include_page_breaks=False)\n",
    "    extracted_text = \"\\n\".join(str(el) for el in raw_data if el).strip()\n",
    "\n",
    "    if not extracted_text:\n",
    "        logging.info(f\"‚ö†Ô∏è No text found in fast extraction, using hi_res OCR for {pdf_path}\")\n",
    "        raw_data = partition_pdf(filename=str(pdf_path), strategy=\"hi_res\", extract_images_in_pdf=True)\n",
    "        extracted_text = \"\\n\".join(str(el) for el in raw_data if el).strip()\n",
    "\n",
    "    cleaned_text = clean_pdf_text(extracted_text)\n",
    "    lang = detect_language(cleaned_text) if cleaned_text else \"unknown\"\n",
    "\n",
    "    logging.info(f\"üî§ Detected language: {lang}\")\n",
    "    logging.info(f\"üÜî Indexing found: {indexing}\")\n",
    "\n",
    "    return cleaned_text, lang, indexing\n",
    "\n",
    "# ---------------------- Summary Generation ----------------------\n",
    "def generate_summary(extracted_text: str, max_tokens: int = 1000) -> dict:\n",
    "    \"\"\"Generate both embedding and client summaries.\"\"\"\n",
    "    try:\n",
    "        splitter = RecursiveCharacterTextSplitter(chunk_size=4000, chunk_overlap=500)\n",
    "        docs = splitter.create_documents([extracted_text])\n",
    "\n",
    "        chunk_summaries = []\n",
    "        for doc in docs:\n",
    "            input_text = chunk_prompt.format(text=doc.page_content)\n",
    "            out = llm.invoke(input_text).strip()\n",
    "            if out:\n",
    "                chunk_summaries.append(out)\n",
    "\n",
    "        if not chunk_summaries:\n",
    "            return {\"embedding_text\": \"NA\", \"client_summary\": \"NA\"}\n",
    "\n",
    "        # Embedding text: dense factual content\n",
    "        embedding_text = \"\\n\".join(chunk_summaries)\n",
    "\n",
    "        # Client summary: final high-level summary\n",
    "        truncated_text = embedding_text[:max_tokens * 4]\n",
    "        input_text_final = final_prompt.format(text=truncated_text)\n",
    "        final_out = llm.invoke(input_text_final).strip()\n",
    "\n",
    "        return {\n",
    "            \"embedding_text\": embedding_text if embedding_text else \"NA\",\n",
    "            \"client_summary\": final_out if final_out else \"NA\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logging.error(f\"‚ùå LLM summarization failed: {e}\")\n",
    "        return {\"embedding_text\": \"NA\", \"client_summary\": \"NA\"}\n",
    "\n",
    "# ---------------------- Excel Handling ----------------------\n",
    "def update_excel(row: pd.Series):\n",
    "    vertical = row[\"Verticals\"]\n",
    "    subcategory = row[\"SubCategory\"]\n",
    "    excel_path = OUTPUT_EXCEL_DIR / f\"{vertical}.xlsx\"\n",
    "\n",
    "    if excel_path.exists():\n",
    "        wb = load_workbook(excel_path)\n",
    "    else:\n",
    "        wb = Workbook()\n",
    "        wb.remove(wb.active)\n",
    "\n",
    "    if subcategory in wb.sheetnames:\n",
    "        ws = wb[subcategory]\n",
    "    else:\n",
    "        ws = wb.create_sheet(title=subcategory)\n",
    "        ws.append(list(row.index))\n",
    "\n",
    "    ws.append([row[h] if row[h] != \"\" else \"NA\" for h in row.index])\n",
    "    wb.save(excel_path)\n",
    "    wb.close()\n",
    "    logging.info(f\"‚úÖ Updated Excel: {excel_path}, Sheet: {subcategory}\")\n",
    "\n",
    "# ---------------------- Single PDF Processing ----------------------\n",
    "def process_single_pdf(row: pd.Series):\n",
    "    pdf_path = row[\"Path\"]\n",
    "    try:\n",
    "        extracted_text, lang, indexing = extract_pdf_text(pdf_path)\n",
    "        row[\"Indexing\"] = indexing\n",
    "\n",
    "        if extracted_text:\n",
    "            if lang in [\"english\", \"mixed\"]:\n",
    "                english_text = filter_english_text(extracted_text)\n",
    "                summaries = generate_summary(english_text)\n",
    "                row[\"Summary\"] = summaries[\"client_summary\"]\n",
    "                row[\"EmbeddingText\"] = summaries[\"embedding_text\"]\n",
    "            elif lang == \"hindi\":\n",
    "                row[\"Summary\"], row[\"EmbeddingText\"] = \"FULL_HINDI\", \"NA\"\n",
    "            else:\n",
    "                row[\"Summary\"], row[\"EmbeddingText\"] = \"NA\", \"NA\"\n",
    "        else:\n",
    "            row[\"Summary\"], row[\"EmbeddingText\"] = \"NA\", \"NA\"\n",
    "    except Exception as e:\n",
    "        logging.error(f\"‚ùå Error processing {pdf_path}: {e}\")\n",
    "        row[\"Summary\"], row[\"EmbeddingText\"], row[\"Indexing\"] = \"NA\", \"NA\", \"NA\"\n",
    "    return row\n",
    "\n",
    "# ---------------------- Main Pipeline ----------------------\n",
    "def main(excel_file: str):\n",
    "    excel_file = Path(excel_file)\n",
    "    if not excel_file.exists():\n",
    "        raise FileNotFoundError(f\"{excel_file} not found!\")\n",
    "\n",
    "    df = pd.read_excel(excel_file)\n",
    "    required_cols = [\"Verticals\", \"SubCategory\", \"Path\"]\n",
    "    for col in required_cols:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"Excel must have '{col}' column\")\n",
    "\n",
    "    if \"Indexing\" not in df.columns:\n",
    "        df[\"Indexing\"] = \"\"\n",
    "    if \"Summary\" not in df.columns:\n",
    "        df[\"Summary\"] = \"\"\n",
    "    if \"EmbeddingText\" not in df.columns:\n",
    "        df[\"EmbeddingText\"] = \"\"\n",
    "\n",
    "    total = len(df)\n",
    "    logging.info(f\"üöÄ Starting sequential PDF processing ({total} PDFs total)...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        pdf_start = time.time()\n",
    "        logging.info(f\"üìò Processing PDF {idx + 1}/{total}: {row['Path']}\")\n",
    "        processed_row = process_single_pdf(row)\n",
    "        update_excel(processed_row)\n",
    "        pdf_end = time.time()\n",
    "        logging.info(f\"‚è±Ô∏è PDF {idx + 1} processed in {pdf_end - pdf_start:.2f} seconds\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    logging.info(f\"üéâ All PDFs processed successfully in {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# ---------------------- Entry ----------------------\n",
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "\n",
    "    if len(sys.argv) < 2:\n",
    "        print(\"‚ùå Usage: python client_summary.py <excel_file_path>\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    excel_file = sys.argv[1]\n",
    "\n",
    "    print(f\"\\nüöÄ Starting PDF summarization pipeline for: {excel_file}\\n\")\n",
    "    logging.info(f\"üìÇ Input Excel: {excel_file}\")\n",
    "\n",
    "    try:\n",
    "        main(excel_file)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"‚ùå Fatal error in main(): {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823def30",
   "metadata": {},
   "source": [
    "# Searching_agent.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447ba931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================================\n",
    "# CRITICAL FIX FOR WINDOWS - MUST BE AT THE VERY TOP OF THE SCRIPT\n",
    "# =========================================================================\n",
    "import sys\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "import platform\n",
    "import os\n",
    "\n",
    "if sys.platform.startswith(\"win\"):\n",
    "    asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())\n",
    "    nest_asyncio.apply()\n",
    "# =========================================================================\n",
    "\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from crawl4ai import AsyncWebCrawler\n",
    "from bs4 import BeautifulSoup\n",
    "import aiohttp\n",
    "import base64\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# -------- CONFIG --------\n",
    "BASE_URL = \"https://www.sebi.gov.in\"\n",
    "LISTING_URL = \"https://www.sebi.gov.in/sebiweb/home/HomeAction.do?doListing=yes&sid=6&ssid=23&smid=0\"\n",
    "CATEGORY = \"SEBI\"\n",
    "SUBFOLDER = \"Press Release\"\n",
    "\n",
    "if platform.system() == \"Windows\":\n",
    "    BASE_PATH = r\"C:\\Users\\Admin\\Downloads\\Tejomaya_pdfs\\Akshayam Data\"\n",
    "else:\n",
    "    BASE_PATH = \"/Users/admin/Downloads/Tejomaya_pdfs/Akshayam Data\"\n",
    "\n",
    "# GLOBAL LIST FOR FINAL EXCEL\n",
    "ALL_DOWNLOADED = []\n",
    "\n",
    "# -------- WEEK RANGE LOGIC --------\n",
    "def get_week_range(weeks_back=0):\n",
    "    today = datetime.today()\n",
    "\n",
    "    this_monday = (today - timedelta(days=today.weekday())).replace(\n",
    "        hour=0, minute=0, second=0, microsecond=0\n",
    "    )\n",
    "\n",
    "    target_monday = this_monday - timedelta(weeks=weeks_back)\n",
    "\n",
    "    target_sunday = (target_monday + timedelta(days=6)).replace(\n",
    "        hour=23, minute=59, second=59, microsecond=999999\n",
    "    )\n",
    "\n",
    "    # Cap to today if checking THIS week\n",
    "    if weeks_back == 0:\n",
    "        target_sunday = today.replace(\n",
    "            hour=23, minute=59, second=59, microsecond=999999\n",
    "        )\n",
    "\n",
    "    print(f\"\\nüìÖ Target range ({weeks_back} week(s) back): {target_monday.date()} ‚Üí {target_sunday.date()}\")\n",
    "    return target_monday, target_sunday\n",
    "\n",
    "\n",
    "# -------- HELPERS --------\n",
    "\n",
    "def sanitize_filename(title):\n",
    "    filename = re.sub(r'[\\/\\\\\\:\\*\\?\"<>\\|]', '_', title)\n",
    "    filename = filename.replace(\" \", \"_\")\n",
    "    return filename + \".pdf\"\n",
    "\n",
    "def ensure_year_month_structure(base_folder, category, subfolder, year, month_full):\n",
    "    subfolder_path = os.path.join(base_folder, category, subfolder)\n",
    "    year_path = os.path.join(subfolder_path, year)\n",
    "    os.makedirs(year_path, exist_ok=True)\n",
    "    month_path = os.path.join(year_path, month_full)\n",
    "    os.makedirs(month_path, exist_ok=True)\n",
    "    return month_path\n",
    "\n",
    "async def download_pdf(session, pdf_url, save_path):\n",
    "    filename = os.path.basename(urlparse(pdf_url).path)\n",
    "    file_path = os.path.join(save_path, filename)\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"‚è≠Ô∏è Skipping (exists): {file_path}\")\n",
    "        return file_path\n",
    "    \n",
    "    try:\n",
    "        async with session.get(pdf_url) as resp:\n",
    "            if resp.status == 200:\n",
    "                with open(file_path, \"wb\") as f:\n",
    "                    f.write(await resp.read())\n",
    "                print(f\"üì• PDF downloaded: {file_path}\")\n",
    "                return file_path\n",
    "            else:\n",
    "                print(f\"‚ùå Failed PDF download ({resp.status})\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error downloading PDF: {e}\")\n",
    "    return None\n",
    "\n",
    "def sync_fetch_links(page_number):\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--headless\")\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    titles_and_urls = []\n",
    "    try:\n",
    "        driver.get(LISTING_URL)\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"a.points\"))\n",
    "        )\n",
    "\n",
    "        if page_number > 1:\n",
    "            page_index = page_number - 1\n",
    "            driver.execute_script(f\"searchFormNewsList('n', '{page_index}');\")\n",
    "            WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_all_elements_located((By.CSS_SELECTOR, \"a.points\"))\n",
    "            )\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "        for link in soup.select(\"a.points[href]\"):\n",
    "            detail_url = link.get(\"href\")\n",
    "            title = link.get_text(strip=True)\n",
    "            if detail_url:\n",
    "                titles_and_urls.append({\n",
    "                    \"title\": title,\n",
    "                    \"detail_url\": urljoin(BASE_URL, detail_url)\n",
    "                })\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "    return titles_and_urls\n",
    "\n",
    "\n",
    "# -------- MAIN SCRAPER --------\n",
    "\n",
    "async def scrape_sebi_page(page_number: int, week_start, week_end):\n",
    "    print(f\"\\n--- Processing SEBI Page {page_number} ---\")\n",
    "\n",
    "    links_data = await asyncio.to_thread(sync_fetch_links, page_number)\n",
    "    if not links_data:\n",
    "        print(\"‚ùå No titles found\")\n",
    "        return\n",
    "\n",
    "    for link_data in links_data:\n",
    "        title = link_data[\"title\"]\n",
    "        detail_url = link_data[\"detail_url\"]\n",
    "\n",
    "        print(f\"\\nüîó Opening: {detail_url}\")\n",
    "\n",
    "        async with AsyncWebCrawler() as crawler:\n",
    "            detail_result = await crawler.arun(url=detail_url)\n",
    "\n",
    "        soup_detail = BeautifulSoup(detail_result.html, \"html.parser\")\n",
    "\n",
    "        date_elem = soup_detail.select_one(\"h5\")\n",
    "        if not date_elem:\n",
    "            continue\n",
    "\n",
    "        date_str = date_elem.get_text(strip=True)\n",
    "        try:\n",
    "            dt = datetime.strptime(date_str, \"%b %d, %Y\")\n",
    "        except:\n",
    "            print(\"‚ö†Ô∏è Invalid date\")\n",
    "            continue\n",
    "\n",
    "        # WEEK FILTER\n",
    "        if not (week_start <= dt <= week_end):\n",
    "            print(f\"‚è≠Ô∏è Skipping (not in week): {dt.date()}\")\n",
    "            continue\n",
    "\n",
    "        print(f\"‚úÖ MATCH (in week): {dt.date()}\")\n",
    "\n",
    "        year = str(dt.year)\n",
    "        month_full = dt.strftime(\"%B\")\n",
    "        save_path = ensure_year_month_structure(BASE_PATH, CATEGORY, SUBFOLDER, year, month_full)\n",
    "\n",
    "        # PDF extraction\n",
    "        pdf_url = None\n",
    "        iframe = soup_detail.select_one(\"iframe\")\n",
    "        pdf_button = soup_detail.select_one(\"button#download\")\n",
    "\n",
    "        if iframe and \"file=\" in iframe.get(\"src\", \"\"):\n",
    "            pdf_url = iframe[\"src\"].split(\"file=\")[-1]\n",
    "            if not pdf_url.startswith(\"http\"):\n",
    "                pdf_url = urljoin(BASE_URL, pdf_url)\n",
    "\n",
    "        elif pdf_button:\n",
    "            pdf_url = detail_url.replace(\".html\", \".pdf\")\n",
    "\n",
    "        file_path = None\n",
    "\n",
    "        if pdf_url:\n",
    "            async with aiohttp.ClientSession() as session:\n",
    "                file_path = await download_pdf(session, pdf_url, save_path)\n",
    "\n",
    "        if not file_path:\n",
    "            # Print-to-PDF fallback\n",
    "            options = webdriver.ChromeOptions()\n",
    "            options.add_argument(\"--headless\")\n",
    "            driver = webdriver.Chrome(options=options)\n",
    "            driver.get(detail_url)\n",
    "\n",
    "            pdf = driver.execute_cdp_cmd(\"Page.printToPDF\", {\"printBackground\": True})\n",
    "            driver.quit()\n",
    "\n",
    "            pdf_data = base64.b64decode(pdf[\"data\"])\n",
    "            file_path = os.path.join(save_path, sanitize_filename(title))\n",
    "\n",
    "            with open(file_path, \"wb\") as f:\n",
    "                f.write(pdf_data)\n",
    "\n",
    "            print(f\"üñ®Ô∏è Printed to PDF: {file_path}\")\n",
    "\n",
    "        # ADD TO GLOBAL LIST\n",
    "        ALL_DOWNLOADED.append({\n",
    "            \"Verticals\": CATEGORY,\n",
    "            \"SubCategory\": SUBFOLDER,\n",
    "            \"Year\": year,\n",
    "            \"Month\": month_full,\n",
    "            \"File Name\": os.path.basename(file_path),\n",
    "            \"Path\": os.path.abspath(file_path)\n",
    "        })\n",
    "\n",
    "\n",
    "# -------- MAIN PROGRAM --------\n",
    "\n",
    "async def main():\n",
    "    weeks_back = 1   # 0=this week, 1=last week\n",
    "    week_start, week_end = get_week_range(weeks_back)\n",
    "\n",
    "    for page in range(1, 3):\n",
    "        await scrape_sebi_page(page, week_start, week_end)\n",
    "\n",
    "    # FINAL EXCEL\n",
    "    if ALL_DOWNLOADED:\n",
    "        df = pd.DataFrame(ALL_DOWNLOADED)\n",
    "        script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "        excel_output = os.path.join(script_dir, \"weekly_sebi_downloads.xlsx\")\n",
    "        \n",
    "        df.to_excel(excel_output, index=False)\n",
    "\n",
    "        print(\"\\nüìÑ FINAL EXCEL GENERATED:\")\n",
    "        print(excel_output)\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è No PDFs found for this week.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38042cef",
   "metadata": {},
   "source": [
    "# parsing_agents.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14aaf624",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from dotenv import load_dotenv\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# from langchain_ollama import OllamaLLM\n",
    "from langchain.llms import Ollama\n",
    "\n",
    "from langchain.prompts import PromptTemplate\n",
    "from openpyxl import load_workbook, Workbook\n",
    "import torch\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "# ---------------------- Setup ----------------------\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    handlers=[logging.StreamHandler()]\n",
    ")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\"No languages specified, defaulting to English.\")\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('averaged_perceptron_tagger', quiet=True)\n",
    "load_dotenv()\n",
    "\n",
    "# ---------------------- GPU Detection ----------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "os.environ[\"OLLAMA_USE_GPU\"] = \"1\" if device.type == \"cuda\" else \"0\"\n",
    "logging.info(f\"üí™ Using device: {device}\")\n",
    "logging.info(\"Ollama GPU status: \" + (\"Enabled\" if os.environ.get(\"OLLAMA_USE_GPU\") == \"1\" else \"Disabled\"))\n",
    "\n",
    "# ---------------------- Initialize LLM ----------------------\n",
    "# llm = OllamaLLM(\n",
    "#     model=\"mistral:latest\",\n",
    "#     device=device.type\n",
    "# )\n",
    "from langchain.llms import Ollama\n",
    "\n",
    "llm = Ollama(model=\"mistral:latest\")\n",
    "\n",
    "\n",
    "chunk_prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    You are analyzing a regulatory or financial document.\n",
    "    Summarize the following section in a **concise and factual** manner (2 sentences max).\n",
    "\n",
    "    Focus only on:\n",
    "    - The key regulatory or policy action (amendment, relaxation, proposal, or update)\n",
    "    - Who or what it affects\n",
    "    - Any critical dates or deadlines mentioned\n",
    "    \n",
    "    Avoid:\n",
    "    - Legal citations, section numbers, or procedural details\n",
    "    - Repetition of headers, page numbers, or contact information\n",
    "    - Explanations, reasoning, or generic filler text\n",
    "\n",
    "    Chunk Text:\n",
    "    {text}\n",
    "\n",
    "    Short factual summary:\n",
    "    \"\"\",\n",
    "    input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "final_prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "    You are a senior communications analyst summarizing official regulatory or legal documents\n",
    "    for a client update. Create a concise, plain-language summary.\n",
    "\n",
    "    Guidelines:\n",
    "    - Limit strictly to 5‚Äì6 sentences.\n",
    "    - Focus only on the core purpose, key changes, and who it applies to.\n",
    "    - Avoid mentioning:\n",
    "        ‚Ä¢ Emails, phone numbers, or submission instructions\n",
    "        ‚Ä¢ Application processes, forms, or deadlines\n",
    "        ‚Ä¢ Section or clause numbers unless critical for understanding\n",
    "        ‚Ä¢ Legal or technical jargon (use simple terms instead)\n",
    "    - Keep the tone professional and factual, not legalistic.\n",
    "    - Do NOT repeat phrases or include disclaimers.\n",
    "\n",
    "    Example Output:\n",
    "    \"SEBI has proposed amendments to the Listing Regulations to simplify compliance and ensure\n",
    "    new securities issued through splits or schemes are in dematerialized form. The consultation\n",
    "    seeks stakeholder feedback to improve market transparency and efficiency.\"\n",
    "\n",
    "    Text:\n",
    "    {text}\n",
    "\n",
    "    Short Client Summary (entire answer MUST be inside double quotes):\n",
    "    \"{{ summary }}\"\n",
    "\n",
    "    \"\"\",\n",
    "    input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "\n",
    "# ---------------------- Folders ----------------------\n",
    "OUTPUT_EXCEL_DIR = Path.cwd() / \"output_excels\"\n",
    "OUTPUT_EXCEL_DIR.mkdir(exist_ok=True)\n",
    "BASE_PDF_FOLDER_NAME = \"All Data\"\n",
    "\n",
    "# ---------------------- PDF Cleaning ----------------------\n",
    "def clean_pdf_text(text: str) -> str:\n",
    "    \"\"\"Remove headers, footers, page numbers, and repeated SEBI-like content.\"\"\"\n",
    "    # Split by page (form feed or manual detection)\n",
    "    pages = re.split(r'\\f+', text)\n",
    "    cleaned_pages = []\n",
    "\n",
    "    for page in pages:\n",
    "        lines = page.splitlines()\n",
    "        # Remove first 1‚Äì2 and last 1‚Äì2 lines (common headers/footers)\n",
    "        if len(lines) > 4:\n",
    "            lines = lines[2:-2]\n",
    "        page_text = \"\\n\".join(lines)\n",
    "\n",
    "        # Remove page numbers like 'Page 3 of 10'\n",
    "        page_text = re.sub(r'Page\\s*\\d+\\s*(of\\s*\\d+)?', '', page_text, flags=re.IGNORECASE)\n",
    "\n",
    "        # Remove repeated SEBI headers/footers\n",
    "        page_text = re.sub(r'(Securities and Exchange Board of India|Consultation Paper|Master Circular)', '', page_text, flags=re.IGNORECASE)\n",
    "\n",
    "        cleaned_pages.append(page_text.strip())\n",
    "\n",
    "    # Merge pages\n",
    "    cleaned_text = \"\\n\".join(cleaned_pages)\n",
    "\n",
    "    # Remove extra spaces/newlines\n",
    "    cleaned_text = re.sub(r'\\n{2,}', '\\n', cleaned_text)\n",
    "    cleaned_text = re.sub(r'\\s{2,}', ' ', cleaned_text)\n",
    "\n",
    "    return cleaned_text.strip()\n",
    "\n",
    "# ---------------------- Language Detection ----------------------\n",
    "def detect_language(text: str) -> str:\n",
    "    devanagari_chars = re.findall(r'[\\u0900-\\u097F]', text)\n",
    "    latin_chars = re.findall(r'[A-Za-z]', text)\n",
    "    d_len, l_len = len(devanagari_chars), len(latin_chars)\n",
    "    total = d_len + l_len\n",
    "    if total == 0:\n",
    "        return \"unknown\"\n",
    "    hindi_ratio = d_len / total\n",
    "    english_ratio = l_len / total\n",
    "    if hindi_ratio > 0.3 and english_ratio > 0.3:\n",
    "        return \"mixed\"\n",
    "    elif hindi_ratio > 0.3:\n",
    "        return \"hindi\"\n",
    "    elif english_ratio > 0.3:\n",
    "        return \"english\"\n",
    "    else:\n",
    "        return \"unknown\"\n",
    "\n",
    "# ---------------------- English Filtering ----------------------\n",
    "def filter_english_text(text: str) -> str:\n",
    "    lines = text.split('\\n')\n",
    "    english_lines = [line for line in lines if re.search(r'[A-Za-z]', line) and not re.search(r'[\\u0900-\\u097F]', line)]\n",
    "    return \"\\n\".join(english_lines)\n",
    "\n",
    "# ---------------------- Index Extraction ----------------------\n",
    "def extract_indexing_from_first_page(pdf_path: str) -> str:\n",
    "    try:\n",
    "        first_page_data = partition_pdf(\n",
    "            filename=str(pdf_path),\n",
    "            strategy=\"fast\",\n",
    "            include_page_breaks=False,\n",
    "            starting_page_number=1,\n",
    "            max_pages=1\n",
    "        )\n",
    "        first_page_text = \"\\n\".join(str(el) for el in first_page_data if el)\n",
    "        #pattern = r'\\b(?:REGD\\.?\\s*NO\\.?\\s*[A-Z.\\s\\-]*\\d+(?:/\\d+)*|IBBI/\\d{4}-\\d{2}/GN/REG\\d{3})\\b'\n",
    "        pattern = r\"\"\"\n",
    "        \\b(\n",
    "            REGD\\.?\\s*No\\.?\\s*[A-Z.\\-\\s]*\\d+(?:/\\d+)* |\n",
    "            (?:CG|DL|MH|HR|UP|GJ|TN|RJ|KL|KA|WB|PB|CH|UK|AS|OR|BR|AP|TS|HP|GA|JK|NL|MN|TR|SK|AR)-[A-Z]{2}-E-\\d{8}-\\d+ |\n",
    "            No\\.?\\s*[A-Z/.\\-]*\\d+(?:/\\d+)* |\n",
    "            SEBI/[A-Z]{2,}/\\d{2}/\\d{2} |\n",
    "            S\\.O\\.\\s*\\d+\\(E\\) |\n",
    "            IBBI/\\d{4}-\\d{2}/GN/REG\\d+\n",
    "        )\\b\n",
    "        \"\"\"\n",
    "        matches = re.findall(pattern, first_page_text, flags=re.VERBOSE)\n",
    "\n",
    "        #matches = re.findall(pattern, first_page_text)\n",
    "        return \", \".join(matches) if matches else \"NA\"\n",
    "    except Exception as e:\n",
    "        logging.warning(f\"‚ö†Ô∏è Could not extract indexing info from {pdf_path}: {e}\")\n",
    "        return \"NA\"\n",
    "\n",
    "# ---------------------- PDF Extraction ----------------------\n",
    "def extract_pdf_text(pdf_path: str) -> (str, str, str):\n",
    "    pdf_path = Path(pdf_path).resolve()\n",
    "    indexing = extract_indexing_from_first_page(pdf_path)\n",
    "\n",
    "    raw_data = partition_pdf(filename=str(pdf_path), strategy=\"fast\", include_page_breaks=False)\n",
    "    extracted_text = \"\\n\".join(str(el) for el in raw_data if el).strip()\n",
    "\n",
    "    if not extracted_text:\n",
    "        logging.info(f\"‚ö†Ô∏è No text found in fast extraction, using hi_res OCR for {pdf_path}\")\n",
    "        raw_data = partition_pdf(filename=str(pdf_path), strategy=\"hi_res\", extract_images_in_pdf=True)\n",
    "        extracted_text = \"\\n\".join(str(el) for el in raw_data if el).strip()\n",
    "\n",
    "    cleaned_text = clean_pdf_text(extracted_text)\n",
    "    lang = detect_language(cleaned_text) if cleaned_text else \"unknown\"\n",
    "\n",
    "    logging.info(f\"üî§ Detected language: {lang}\")\n",
    "    logging.info(f\"üÜî Indexing found: {indexing}\")\n",
    "\n",
    "    return cleaned_text, lang, indexing\n",
    "\n",
    "# ---------------------- Summary Generation ----------------------\n",
    "def generate_summary(extracted_text: str, max_tokens: int = 1000) -> dict:\n",
    "    \"\"\"Generate both embedding and client summaries.\"\"\"\n",
    "    try:\n",
    "        splitter = RecursiveCharacterTextSplitter(chunk_size=4000, chunk_overlap=500)\n",
    "        docs = splitter.create_documents([extracted_text])\n",
    "\n",
    "        chunk_summaries = []\n",
    "        for doc in docs:\n",
    "            input_text = chunk_prompt.format(text=doc.page_content)\n",
    "            out = llm.invoke(input_text).strip()\n",
    "            if out:\n",
    "                chunk_summaries.append(out)\n",
    "\n",
    "        if not chunk_summaries:\n",
    "            return {\"embedding_text\": \"NA\", \"client_summary\": \"NA\"}\n",
    "\n",
    "        # Embedding text: dense factual content\n",
    "        embedding_text = \"\\n\".join(chunk_summaries)\n",
    "\n",
    "        # Client summary: final high-level summary\n",
    "        truncated_text = embedding_text[:max_tokens * 4]\n",
    "        input_text_final = final_prompt.format(text=truncated_text)\n",
    "        final_out = llm.invoke(input_text_final).strip()\n",
    "\n",
    "        return {\n",
    "            \"embedding_text\": embedding_text if embedding_text else \"NA\",\n",
    "            \"client_summary\": final_out if final_out else \"NA\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logging.error(f\"‚ùå LLM summarization failed: {e}\")\n",
    "        return {\"embedding_text\": \"NA\", \"client_summary\": \"NA\"}\n",
    "\n",
    "# ---------------------- Excel Handling ----------------------\n",
    "def update_excel(row: pd.Series):\n",
    "    vertical = row[\"Verticals\"]\n",
    "    subcategory = row[\"SubCategory\"]\n",
    "    excel_path = OUTPUT_EXCEL_DIR / f\"{vertical}.xlsx\"\n",
    "\n",
    "    if excel_path.exists():\n",
    "        wb = load_workbook(excel_path)\n",
    "    else:\n",
    "        wb = Workbook()\n",
    "        wb.remove(wb.active)\n",
    "\n",
    "    if subcategory in wb.sheetnames:\n",
    "        ws = wb[subcategory]\n",
    "    else:\n",
    "        ws = wb.create_sheet(title=subcategory)\n",
    "        ws.append(list(row.index))\n",
    "\n",
    "    ws.append([row[h] if row[h] != \"\" else \"NA\" for h in row.index])\n",
    "    wb.save(excel_path)\n",
    "    wb.close()\n",
    "    logging.info(f\"‚úÖ Updated Excel: {excel_path}, Sheet: {subcategory}\")\n",
    "\n",
    "# ---------------------- Single PDF Processing ----------------------\n",
    "def process_single_pdf(row: pd.Series):\n",
    "    pdf_path = row[\"Path\"]\n",
    "    try:\n",
    "        extracted_text, lang, indexing = extract_pdf_text(pdf_path)\n",
    "        row[\"Indexing\"] = indexing\n",
    "\n",
    "        if extracted_text:\n",
    "            if lang in [\"english\", \"mixed\"]:\n",
    "                english_text = filter_english_text(extracted_text)\n",
    "                summaries = generate_summary(english_text)\n",
    "                row[\"Summary\"] = summaries[\"client_summary\"]\n",
    "                row[\"EmbeddingText\"] = summaries[\"embedding_text\"]\n",
    "            elif lang == \"hindi\":\n",
    "                row[\"Summary\"], row[\"EmbeddingText\"] = \"FULL_HINDI\", \"NA\"\n",
    "            else:\n",
    "                row[\"Summary\"], row[\"EmbeddingText\"] = \"NA\", \"NA\"\n",
    "        else:\n",
    "            row[\"Summary\"], row[\"EmbeddingText\"] = \"NA\", \"NA\"\n",
    "    except Exception as e:\n",
    "        logging.error(f\"‚ùå Error processing {pdf_path}: {e}\")\n",
    "        row[\"Summary\"], row[\"EmbeddingText\"], row[\"Indexing\"] = \"NA\", \"NA\", \"NA\"\n",
    "    return row\n",
    "\n",
    "# ---------------------- Main Pipeline ----------------------\n",
    "def main(excel_file: str):\n",
    "    excel_file = Path(excel_file)\n",
    "    if not excel_file.exists():\n",
    "        raise FileNotFoundError(f\"{excel_file} not found!\")\n",
    "\n",
    "    df = pd.read_excel(excel_file)\n",
    "    required_cols = [\"Verticals\", \"SubCategory\", \"Path\"]\n",
    "    for col in required_cols:\n",
    "        if col not in df.columns:\n",
    "            raise ValueError(f\"Excel must have '{col}' column\")\n",
    "\n",
    "    if \"Indexing\" not in df.columns:\n",
    "        df[\"Indexing\"] = \"\"\n",
    "    if \"Summary\" not in df.columns:\n",
    "        df[\"Summary\"] = \"\"\n",
    "    if \"EmbeddingText\" not in df.columns:\n",
    "        df[\"EmbeddingText\"] = \"\"\n",
    "\n",
    "    total = len(df)\n",
    "    logging.info(f\"üöÄ Starting sequential PDF processing ({total} PDFs total)...\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        pdf_start = time.time()\n",
    "        logging.info(f\"üìò Processing PDF {idx + 1}/{total}: {row['Path']}\")\n",
    "        processed_row = process_single_pdf(row)\n",
    "        update_excel(processed_row)\n",
    "        pdf_end = time.time()\n",
    "        logging.info(f\"‚è±Ô∏è PDF {idx + 1} processed in {pdf_end - pdf_start:.2f} seconds\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    logging.info(f\"üéâ All PDFs processed successfully in {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# ---------------------- Entry ----------------------\n",
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "\n",
    "    # Auto-detect the excel from scraper output\n",
    "    default_excel = Path.cwd() / \"weekly_sebi_downloads.xlsx\"\n",
    "\n",
    "    if len(sys.argv) >= 2:\n",
    "        excel_file = Path(sys.argv[1])\n",
    "        print(f\"üìÇ Using Excel from CLI: {excel_file}\")\n",
    "    elif default_excel.exists():\n",
    "        excel_file = default_excel\n",
    "        print(f\"üìÇ Auto-detected Excel: {excel_file}\")\n",
    "    else:\n",
    "        print(\"‚ùå No Excel file provided and weekly_sebi_downloads.xlsx not found!\")\n",
    "        print(\"Usage: python process_pdfs.py <excel_file_path>\")\n",
    "        sys.exit(1)\n",
    "\n",
    "\n",
    "    print(f\"\\nüöÄ Starting PDF summarization pipeline for: {excel_file}\\n\")\n",
    "    logging.info(f\"üìÇ Input Excel: {excel_file}\")\n",
    "\n",
    "    try:\n",
    "        main(excel_file)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"‚ùå Fatal error in main(): {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb1c3c5",
   "metadata": {},
   "source": [
    "# run_agents.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b594b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import subprocess\n",
    "import time\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s %(levelname)s %(message)s\"\n",
    ")\n",
    "\n",
    "PYTHON = sys.executable\n",
    "SEARCH_AGENT = str(Path(\"Searching_agent.py\").resolve())\n",
    "PARSE_AGENT = str(Path(\"Parsing_agent.py\").resolve())\n",
    "\n",
    "SEARCH_LOG = str(Path(\"searching_agent.log\").resolve())\n",
    "PARSE_LOG = str(Path(\"parsing_agent.log\").resolve())\n",
    "\n",
    "EXCEL_OUTPUT = Path(\"weekly_sebi_downloads.xlsx\")\n",
    "\n",
    "\n",
    "def run_process(script_path, log_path):\n",
    "    \"\"\"Runs a script and logs output to file.\"\"\"\n",
    "    logging.info(f\"üöÄ Starting script: {script_path}\")\n",
    "    with open(log_path, \"a\") as f:\n",
    "        f.write(f\"\\n\\n=== RUN AT {time.strftime('%Y-%m-%d %H:%M:%S')} ===\\n\")\n",
    "        result = subprocess.run([PYTHON, script_path], stdout=f, stderr=f)\n",
    "    logging.info(f\"‚úÖ Finished {script_path} with returncode={result.returncode}\")\n",
    "    return result.returncode\n",
    "\n",
    "\n",
    "def main():\n",
    "    logging.info(\"=============== STARTING AGENT CHAIN ===============\")\n",
    "\n",
    "    # 1Ô∏è‚É£ Run Searching Agent\n",
    "    rc1 = run_process(SEARCH_AGENT, SEARCH_LOG)\n",
    "    if rc1 != 0:\n",
    "        logging.error(\"‚ùå Searching Agent failed. Stopping chain.\")\n",
    "        return\n",
    "\n",
    "    # 2Ô∏è‚É£ Check if Excel exists\n",
    "    if not EXCEL_OUTPUT.exists():\n",
    "        logging.error(f\"‚ùå Excel not found: {EXCEL_OUTPUT}. Cannot run Parsing Agent.\")\n",
    "        return\n",
    "\n",
    "    logging.info(f\"üìÑ Excel found --> {EXCEL_OUTPUT}. Proceeding to Parsing Agent.\")\n",
    "\n",
    "    # 3Ô∏è‚É£ Run Parsing Agent\n",
    "    rc2 = run_process(PARSE_AGENT, PARSE_LOG)\n",
    "    if rc2 != 0:\n",
    "        logging.error(\"‚ùå Parsing Agent failed!\")\n",
    "        return\n",
    "\n",
    "    logging.info(\"üéâ BOTH AGENTS COMPLETED SUCCESSFULLY!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837e74c0",
   "metadata": {},
   "source": [
    "# scheduler.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd830b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import schedule\n",
    "import time\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s %(levelname)s %(message)s\")\n",
    "\n",
    "# ------------- CONFIG -------------\n",
    "# If you want the scheduler to use the same Python interpreter you launched it with,\n",
    "# leave PYTHON_PATH = sys.executable. Otherwise set explicit interpreter path.\n",
    "PYTHON_PATH = sys.executable  # uses the current Python (recommended)\n",
    "SCRIPT_NAME = \"Searching_agent.py\"  # <-- change to \"Parsing_agent.py\" if needed\n",
    "\n",
    "SCRIPT_PATH = str(Path(__file__).parent.joinpath(SCRIPT_NAME).resolve())\n",
    "# LOG file for agent stdout/stderr\n",
    "LOG_PATH = str(Path(__file__).parent.joinpath(\"agent_run.log\").resolve())\n",
    "# ----------------------------------\n",
    "\n",
    "def run_agent():\n",
    "    logging.info(\"Starting agent: %s\", SCRIPT_PATH)\n",
    "    # Use subprocess.run to wait and capture return code; redirect stdout/stderr to log file\n",
    "    with open(LOG_PATH, \"a\") as f:\n",
    "        f.write(\"\\n\\n=== Run at: {} ===\\n\".format(time.strftime(\"%Y-%m-%d %H:%M:%S\")))\n",
    "        try:\n",
    "            # On Windows, shell=False; provide full interpreter + script\n",
    "            result = subprocess.run([PYTHON_PATH, SCRIPT_PATH], stdout=f, stderr=f, check=False)\n",
    "            logging.info(\"Agent finished with returncode=%s\", result.returncode)\n",
    "        except Exception as e:\n",
    "            logging.exception(\"Failed to run agent: %s\", e)\n",
    "\n",
    "# Examples: schedule as needed\n",
    "# schedule.every().day.at(\"09:00\").do(run_agent)           # daily 09:00\n",
    "\n",
    "# schedule.every().day.at(\"12:59\").do(run_agent)\n",
    "\n",
    "def run_agents():\n",
    "    subprocess.Popen([sys.executable, \"run_agents.py\"])\n",
    "\n",
    "schedule.every().day.at(\"13:12\").do(run_agents)\n",
    "\n",
    "# schedule.every().monday.at(\"07:00\").do(run_agent)     # weekly Monday 07:00\n",
    "# schedule.every().hour.do(run_agent)                   # every hour\n",
    "\n",
    "logging.info(\"Scheduler started. Agent script: %s\", SCRIPT_PATH)\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        schedule.run_pending()\n",
    "        time.sleep(1)\n",
    "except KeyboardInterrupt:\n",
    "    logging.info(\"Scheduler stopped by user.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tejomaya",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
